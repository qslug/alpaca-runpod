{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speedtest\n",
    "st = speedtest.Speedtest()\n",
    "print(f\"Your download speed: {round(st.download() / 1000 / 1000, 1)} Mbit/s\")\n",
    "print(f\"Your upload speed: {round(st.upload() / 1000 / 1000, 1)} Mbit/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun train.py \\\n",
    "    --model_name_or_path \"decapoda-research/llama-7b-hf\" \\\n",
    "    --data_path ./alpaca_data.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir output \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 2000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --fsdp \"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\n",
    "    --tf32 True"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
